{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c28aed0",
   "metadata": {},
   "source": [
    "# Lab 4: Image Segmentation and Object Detection\n",
    "\n",
    "This lab covers the fundamentals of image segmentation and object detection, from traditional methods to modern deep learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2129ae3",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will understand:\n",
    "- The differences between object detection, instance segmentation, and semantic segmentation\n",
    "- Tools for image annotation and label generation\n",
    "- Traditional segmentation methods (atlas-based, thresholding, watershed)\n",
    "- Modern deep learning approaches to segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ab419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndi\n",
    "from skimage import filters, measure, morphology, segmentation\n",
    "from skimage.feature import peak_local_maxima\n",
    "from scipy import ndimage\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# For visualization\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58147800",
   "metadata": {},
   "source": [
    "## 1. Object Detection vs Instance Segmentation vs Semantic Segmentation\n",
    "\n",
    "Let's start by understanding the key differences between these three computer vision tasks:\n",
    "\n",
    "### Object Detection\n",
    "- **Goal**: Locate and classify objects in images\n",
    "- **Output**: Bounding boxes + class labels\n",
    "- **Example**: \"There are 3 cars at positions (x1,y1,w1,h1), (x2,y2,w2,h2), (x3,y3,w3,h3)\"\n",
    "\n",
    "### Semantic Segmentation\n",
    "- **Goal**: Classify every pixel in the image\n",
    "- **Output**: Pixel-wise class labels (same class = same label)\n",
    "- **Example**: All car pixels are labeled as \"car\", but individual cars are not distinguished\n",
    "\n",
    "### Instance Segmentation\n",
    "- **Goal**: Classify every pixel AND distinguish between instances\n",
    "- **Output**: Pixel-wise labels + instance IDs\n",
    "- **Example**: Car 1 pixels, Car 2 pixels, Car 3 pixels (each car gets unique ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d78c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display a sample image for demonstration\n",
    "# Replace with your own image path\n",
    "image_path = '../data/sample_cells.png'  # Example: microscopy image with cells\n",
    "\n",
    "try:\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        # Create a synthetic image if file doesn't exist\n",
    "        print(\"Creating synthetic image for demonstration...\")\n",
    "        image = np.zeros((200, 200), dtype=np.uint8)\n",
    "        # Add some circular \"cells\"\n",
    "        cv2.circle(image, (50, 50), 20, 255, -1)\n",
    "        cv2.circle(image, (120, 80), 25, 255, -1)\n",
    "        cv2.circle(image, (80, 150), 18, 255, -1)\n",
    "        cv2.circle(image, (150, 160), 22, 255, -1)\n",
    "        \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Simulate object detection (bounding boxes)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    # Find contours to create bounding boxes\n",
    "    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        plt.plot([x, x+w, x+w, x, x], [y, y, y+h, y+h, y], 'r-', linewidth=2)\n",
    "    plt.title('Object Detection\\n(Bounding Boxes)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Create semantic segmentation mask\n",
    "    plt.subplot(1, 3, 3)\n",
    "    semantic_mask = (image > 127).astype(np.uint8)  # Simple thresholding\n",
    "    plt.imshow(semantic_mask, cmap='viridis')\n",
    "    plt.title('Semantic Segmentation\\n(All objects same class)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading image: {e}\")\n",
    "    print(\"Please ensure you have a sample image or the synthetic image generation works.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a03cda",
   "metadata": {},
   "source": [
    "## 2. Image Annotation Tools\n",
    "\n",
    "Before we can train machine learning models, we need labeled data. Here are popular tools for creating annotations:\n",
    "\n",
    "### Free/Open Source Tools:\n",
    "1. **LabelMe** - Web-based annotation tool\n",
    "2. **CVAT (Computer Vision Annotation Tool)** - Intel's annotation platform\n",
    "3. **VGG Image Annotator (VIA)** - Lightweight web-based tool\n",
    "4. **Labelbox** - Has free tier\n",
    "5. **COCO Annotator** - For COCO format datasets\n",
    "\n",
    "### Commercial Tools:\n",
    "1. **Supervisely**\n",
    "2. **V7 Darwin**\n",
    "3. **Scale AI**\n",
    "4. **Amazon SageMaker Ground Truth**\n",
    "\n",
    "### Medical Imaging Specific:\n",
    "1. **3D Slicer** - Free, powerful for 3D medical images\n",
    "2. **ITK-SNAP** - Free, good for 3D segmentation\n",
    "3. **MITK** - Research platform with annotation capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ead2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate annotation format examples\n",
    "print(\"=== Common Annotation Formats ===\\n\")\n",
    "\n",
    "print(\"1. COCO Format (Object Detection):\")\n",
    "coco_example = {\n",
    "    \"id\": 1,\n",
    "    \"image_id\": 123,\n",
    "    \"category_id\": 1,\n",
    "    \"bbox\": [x, y, width, height],  # [10, 10, 50, 50]\n",
    "    \"area\": 2500,\n",
    "    \"iscrowd\": 0\n",
    "}\n",
    "print(coco_example)\n",
    "\n",
    "print(\"\\n2. YOLO Format (Object Detection):\")\n",
    "print(\"class_id x_center y_center width height (normalized 0-1)\")\n",
    "print(\"0 0.5 0.5 0.2 0.3\")\n",
    "\n",
    "print(\"\\n3. Semantic Segmentation:\")\n",
    "print(\"- Each pixel has a class label (0=background, 1=car, 2=person, etc.)\")\n",
    "print(\"- Stored as grayscale images where pixel values = class IDs\")\n",
    "\n",
    "print(\"\\n4. Instance Segmentation:\")\n",
    "print(\"- Each pixel has instance ID (0=background, 1=car1, 2=car2, 3=person1, etc.)\")\n",
    "print(\"- Or separate binary masks for each instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c440095",
   "metadata": {},
   "source": [
    "## 3. Traditional Segmentation Methods\n",
    "\n",
    "Before deep learning, computer vision relied on traditional image processing techniques. Let's explore three fundamental approaches:\n",
    "\n",
    "### 3.1 Atlas-Based Segmentation\n",
    "Uses a reference template (atlas) to segment new images by registration and label transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1205df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atlas-based segmentation example\n",
    "print(\"=== Atlas-Based Segmentation ===\")\n",
    "print(\"This method involves:\")\n",
    "print(\"1. Register (align) the new image to an atlas image\")\n",
    "print(\"2. Transfer the atlas labels to the new image\")\n",
    "print(\"3. Refine the segmentation\")\n",
    "\n",
    "# Simple demonstration with template matching\n",
    "def atlas_based_demo(image):\n",
    "    \"\"\"Simplified atlas-based segmentation demo\"\"\"\n",
    "    # Create a simple \"atlas\" template\n",
    "    template = np.zeros((40, 40), dtype=np.uint8)\n",
    "    cv2.circle(template, (20, 20), 15, 255, -1)\n",
    "    \n",
    "    # Template matching (simplified registration)\n",
    "    result = cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED)\n",
    "    \n",
    "    # Find locations where template matches well\n",
    "    threshold = 0.3\n",
    "    locations = np.where(result >= threshold)\n",
    "    \n",
    "    # Create segmentation mask\n",
    "    mask = np.zeros_like(image)\n",
    "    for pt in zip(*locations[::-1]):\n",
    "        cv2.circle(mask, (pt[0] + 20, pt[1] + 20), 15, 255, -1)\n",
    "    \n",
    "    return mask, result\n",
    "\n",
    "if 'image' in locals():\n",
    "    atlas_mask, match_result = atlas_based_demo(image)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(match_result, cmap='hot')\n",
    "    plt.title('Template Matching Result')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(atlas_mask, cmap='viridis')\n",
    "    plt.title('Atlas-Based Segmentation')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aaf1f3",
   "metadata": {},
   "source": [
    "### 3.2 Thresholding-Based Segmentation\n",
    "Separates objects from background based on intensity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff52ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholding-based segmentation\n",
    "print(\"=== Thresholding-Based Segmentation ===\")\n",
    "\n",
    "if 'image' in locals():\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # 1. Simple thresholding\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 2. Histogram to choose threshold\n",
    "    plt.subplot(2, 4, 2)\n",
    "    plt.hist(image.flatten(), bins=50, alpha=0.7)\n",
    "    plt.title('Intensity Histogram')\n",
    "    plt.xlabel('Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # 3. Manual threshold\n",
    "    manual_thresh = 127\n",
    "    _, binary_manual = cv2.threshold(image, manual_thresh, 255, cv2.THRESH_BINARY)\n",
    "    plt.subplot(2, 4, 3)\n",
    "    plt.imshow(binary_manual, cmap='gray')\n",
    "    plt.title(f'Manual Threshold ({manual_thresh})')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 4. Otsu's automatic thresholding\n",
    "    otsu_thresh, binary_otsu = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    plt.subplot(2, 4, 4)\n",
    "    plt.imshow(binary_otsu, cmap='gray')\n",
    "    plt.title(f'Otsu Threshold ({otsu_thresh:.1f})')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 5. Adaptive thresholding\n",
    "    adaptive_thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                          cv2.THRESH_BINARY, 11, 2)\n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.imshow(adaptive_thresh, cmap='gray')\n",
    "    plt.title('Adaptive Threshold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 6. Multi-level thresholding (using skimage)\n",
    "    thresholds = filters.threshold_multiotsu(image, classes=3)\n",
    "    regions = np.digitize(image, bins=thresholds)\n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.imshow(regions, cmap='viridis')\n",
    "    plt.title('Multi-Otsu (3 classes)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 7. Morphological operations to clean up\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    cleaned = cv2.morphologyEx(binary_otsu, cv2.MORPH_OPEN, kernel)\n",
    "    cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel)\n",
    "    plt.subplot(2, 4, 7)\n",
    "    plt.imshow(cleaned, cmap='gray')\n",
    "    plt.title('Morphologically Cleaned')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 8. Connected components\n",
    "    num_labels, labels = cv2.connectedComponents(cleaned)\n",
    "    plt.subplot(2, 4, 8)\n",
    "    plt.imshow(labels, cmap='tab20')\n",
    "    plt.title(f'Connected Components ({num_labels-1})')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Otsu threshold: {otsu_thresh:.1f}\")\n",
    "    print(f\"Multi-Otsu thresholds: {thresholds}\")\n",
    "    print(f\"Number of connected components: {num_labels-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743bfa1",
   "metadata": {},
   "source": [
    "### 3.3 Watershed-Based Segmentation\n",
    "Treats the image as a topographic map and finds watershed lines to separate objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4973663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watershed segmentation\n",
    "print(\"=== Watershed Segmentation ===\")\n",
    "\n",
    "if 'image' in locals():\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # 1. Original image\n",
    "    plt.subplot(2, 4, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 2. Distance transform\n",
    "    binary = (image > filters.threshold_otsu(image)).astype(np.uint8)\n",
    "    distance = ndi.distance_transform_edt(binary)\n",
    "    plt.subplot(2, 4, 2)\n",
    "    plt.imshow(distance, cmap='hot')\n",
    "    plt.title('Distance Transform')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 3. Find local maxima (seeds)\n",
    "    local_maxima = peak_local_maxima(distance, min_distance=20, threshold_abs=0.3*distance.max())\n",
    "    seeds = np.zeros_like(distance, dtype=bool)\n",
    "    seeds[tuple(local_maxima.T)] = True\n",
    "    \n",
    "    plt.subplot(2, 4, 3)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.plot(local_maxima[:, 1], local_maxima[:, 0], 'r*', markersize=10)\n",
    "    plt.title('Detected Seeds')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 4. Label seeds\n",
    "    markers, _ = ndi.label(seeds)\n",
    "    \n",
    "    plt.subplot(2, 4, 4)\n",
    "    plt.imshow(markers, cmap='tab20')\n",
    "    plt.title('Labeled Seeds')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 5. Watershed segmentation\n",
    "    labels = segmentation.watershed(-distance, markers, mask=binary)\n",
    "    \n",
    "    plt.subplot(2, 4, 5)\n",
    "    plt.imshow(labels, cmap='tab20')\n",
    "    plt.title('Watershed Segmentation')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 6. Watershed boundaries\n",
    "    plt.subplot(2, 4, 6)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.contour(labels, levels=np.unique(labels), colors='red', linewidths=1)\n",
    "    plt.title('Watershed Boundaries')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 7. Alternative: Marker-controlled watershed\n",
    "    # Create markers from morphological operations\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    opening = cv2.morphologyEx(binary*255, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "    _, sure_fg = cv2.threshold(dist_transform, 0.7*dist_transform.max(), 255, 0)\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "    \n",
    "    plt.subplot(2, 4, 7)\n",
    "    plt.imshow(sure_fg, cmap='gray')\n",
    "    plt.title('Sure Foreground')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Apply watershed\n",
    "    _, markers2 = cv2.connectedComponents(sure_fg)\n",
    "    markers2 = markers2 + 1\n",
    "    markers2[unknown == 255] = 0\n",
    "    \n",
    "    # Convert image to 3-channel for watershed\n",
    "    img_3ch = cv2.cvtColor((image).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    markers2 = cv2.watershed(img_3ch, markers2)\n",
    "    img_3ch[markers2 == -1] = [255, 0, 0]  # Mark boundaries in red\n",
    "    \n",
    "    plt.subplot(2, 4, 8)\n",
    "    plt.imshow(cv2.cvtColor(img_3ch, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('OpenCV Watershed')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Number of watershed regions: {len(np.unique(labels)) - 1}\")\n",
    "    print(f\"Seeds found: {len(local_maxima)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876456dd",
   "metadata": {},
   "source": [
    "## 4. Segmentation with Deep Learning\n",
    "\n",
    "Modern deep learning approaches have revolutionized image segmentation. Let's explore the key architectures and concepts:\n",
    "\n",
    "### Popular Deep Learning Architectures:\n",
    "\n",
    "1. **U-Net** - Most popular for medical imaging\n",
    "2. **FCN (Fully Convolutional Networks)** - Pioneer in semantic segmentation\n",
    "3. **DeepLab** - Uses atrous convolution for multi-scale features\n",
    "4. **Mask R-CNN** - Extends Faster R-CNN for instance segmentation\n",
    "5. **YOLO + segmentation heads** - Real-time segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Segmentation Concepts\n",
    "print(\"=== Deep Learning Segmentation Overview ===\\n\")\n",
    "\n",
    "print(\"1. U-Net Architecture:\")\n",
    "print(\"   - Encoder-Decoder structure with skip connections\")\n",
    "print(\"   - Contracting path (downsampling) + Expansive path (upsampling)\")\n",
    "print(\"   - Skip connections preserve fine details\")\n",
    "print(\"   - Originally designed for medical image segmentation\")\n",
    "\n",
    "print(\"\\n2. Key Components:\")\n",
    "print(\"   - Convolutional layers for feature extraction\")\n",
    "print(\"   - Pooling layers for downsampling\")\n",
    "print(\"   - Transpose convolutions for upsampling\")\n",
    "print(\"   - Skip connections to combine features at different scales\")\n",
    "\n",
    "print(\"\\n3. Loss Functions:\")\n",
    "print(\"   - Cross-entropy loss (standard)\")\n",
    "print(\"   - Dice loss (overlap-based)\")\n",
    "print(\"   - Focal loss (handles class imbalance)\")\n",
    "print(\"   - Combined losses (e.g., Dice + Cross-entropy)\")\n",
    "\n",
    "print(\"\\n4. Data Augmentation:\")\n",
    "print(\"   - Rotation, flipping, scaling\")\n",
    "print(\"   - Elastic deformations\")\n",
    "print(\"   - Intensity augmentations\")\n",
    "print(\"   - Cutout/mixup techniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c63c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a simple deep learning preprocessing pipeline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_for_deep_learning(image, target_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Preprocessing steps typically used for deep learning segmentation\n",
    "    \"\"\"\n",
    "    # 1. Resize to standard size\n",
    "    resized = cv2.resize(image, target_size)\n",
    "    \n",
    "    # 2. Normalize to [0, 1]\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    # 3. Standardize (subtract mean, divide by std)\n",
    "    # Using ImageNet statistics as example\n",
    "    mean = 0.485  # For grayscale, use single value\n",
    "    std = 0.229\n",
    "    standardized = (normalized - mean) / std\n",
    "    \n",
    "    # 4. Convert to tensor and add batch/channel dimensions\n",
    "    tensor = torch.from_numpy(standardized).unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)\n",
    "    \n",
    "    return tensor, normalized\n",
    "\n",
    "if 'image' in locals():\n",
    "    # Demonstrate preprocessing\n",
    "    processed_tensor, normalized_img = preprocess_for_deep_learning(image)\n",
    "    \n",
    "    print(\"Preprocessing Results:\")\n",
    "    print(f\"Original shape: {image.shape}\")\n",
    "    print(f\"Tensor shape: {processed_tensor.shape}\")\n",
    "    print(f\"Tensor dtype: {processed_tensor.dtype}\")\n",
    "    print(f\"Tensor range: [{processed_tensor.min():.3f}, {processed_tensor.max():.3f}]\")\n",
    "    \n",
    "    # Visualize preprocessing steps\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f'Original ({image.shape})')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(normalized_img, cmap='gray')\n",
    "    plt.title('Normalized [0,1]')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(processed_tensor.squeeze().numpy(), cmap='gray')\n",
    "    plt.title('Standardized')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing techniques for deep learning outputs\n",
    "def postprocess_segmentation(prediction, original_size, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Common post-processing steps for segmentation outputs\n",
    "    \"\"\"\n",
    "    # 1. Apply threshold (for binary segmentation)\n",
    "    if prediction.max() <= 1.0:  # Probability output\n",
    "        binary = (prediction > threshold).astype(np.uint8)\n",
    "    else:  # Already binary\n",
    "        binary = prediction.astype(np.uint8)\n",
    "    \n",
    "    # 2. Remove small objects\n",
    "    cleaned = morphology.remove_small_objects(binary.astype(bool), min_size=50)\n",
    "    \n",
    "    # 3. Fill holes\n",
    "    filled = ndi.binary_fill_holes(cleaned)\n",
    "    \n",
    "    # 4. Resize back to original size\n",
    "    resized = cv2.resize(filled.astype(np.uint8), original_size[::-1])\n",
    "    \n",
    "    # 5. Connected component analysis\n",
    "    num_labels, labels = cv2.connectedComponents(resized)\n",
    "    \n",
    "    return resized, labels, num_labels-1\n",
    "\n",
    "# Example usage with our thresholded image\n",
    "if 'binary_otsu' in locals():\n",
    "    # Simulate a \"prediction\" by adding some noise and resizing\n",
    "    noisy_pred = binary_otsu.astype(np.float32) / 255.0\n",
    "    noisy_pred += np.random.normal(0, 0.1, noisy_pred.shape)\n",
    "    noisy_pred = np.clip(noisy_pred, 0, 1)\n",
    "    \n",
    "    # Resize to simulate model output size\n",
    "    small_pred = cv2.resize(noisy_pred, (128, 128))\n",
    "    \n",
    "    # Post-process\n",
    "    final_mask, labeled_components, num_objects = postprocess_segmentation(\n",
    "        small_pred, image.shape\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(noisy_pred, cmap='gray')\n",
    "    plt.title('Simulated Raw Prediction')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(small_pred, cmap='gray')\n",
    "    plt.title('Resized Prediction')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(final_mask, cmap='gray')\n",
    "    plt.title('Post-processed Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(labeled_components, cmap='tab20')\n",
    "    plt.title(f'Final Components ({num_objects})')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Detected {num_objects} objects after post-processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470ecd4",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics for Segmentation\n",
    "\n",
    "Understanding how to evaluate segmentation performance is crucial:\n",
    "\n",
    "### Common Metrics:\n",
    "1. **Pixel Accuracy** - Percentage of correctly classified pixels\n",
    "2. **IoU (Intersection over Union)** - Overlap between prediction and ground truth\n",
    "3. **Dice Coefficient** - 2 * |A ∩ B| / (|A| + |B|)\n",
    "4. **Hausdorff Distance** - Maximum distance between boundaries\n",
    "5. **Mean Average Precision (mAP)** - For object detection/instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation evaluation metrics\n",
    "def calculate_segmentation_metrics(pred_mask, true_mask):\n",
    "    \"\"\"\n",
    "    Calculate common segmentation metrics\n",
    "    \"\"\"\n",
    "    # Ensure binary masks\n",
    "    pred_bin = (pred_mask > 0).astype(bool)\n",
    "    true_bin = (true_mask > 0).astype(bool)\n",
    "    \n",
    "    # Intersection and union\n",
    "    intersection = np.logical_and(pred_bin, true_bin).sum()\n",
    "    union = np.logical_or(pred_bin, true_bin).sum()\n",
    "    \n",
    "    # Metrics\n",
    "    pixel_accuracy = np.mean(pred_bin == true_bin)\n",
    "    \n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    \n",
    "    dice = (2 * intersection) / (pred_bin.sum() + true_bin.sum()) if (pred_bin.sum() + true_bin.sum()) > 0 else 0\n",
    "    \n",
    "    # Precision and Recall\n",
    "    tp = intersection\n",
    "    fp = pred_bin.sum() - intersection\n",
    "    fn = true_bin.sum() - intersection\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'pixel_accuracy': pixel_accuracy,\n",
    "        'iou': iou,\n",
    "        'dice': dice,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "# Example evaluation\n",
    "if 'binary_otsu' in locals() and 'cleaned' in locals():\n",
    "    # Use one as \"ground truth\" and other as \"prediction\"\n",
    "    metrics = calculate_segmentation_metrics(cleaned, binary_otsu)\n",
    "    \n",
    "    print(\"=== Segmentation Metrics ===\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(binary_otsu, cmap='gray')\n",
    "    plt.title('Ground Truth')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(cleaned, cmap='gray')\n",
    "    plt.title('Prediction')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show overlap\n",
    "    plt.subplot(1, 3, 3)\n",
    "    overlap_vis = np.zeros((*binary_otsu.shape, 3))\n",
    "    overlap_vis[binary_otsu > 0] = [1, 0, 0]  # Ground truth in red\n",
    "    overlap_vis[cleaned > 0] = [0, 1, 0]  # Prediction in green\n",
    "    overlap_vis[np.logical_and(binary_otsu > 0, cleaned > 0)] = [1, 1, 0]  # Overlap in yellow\n",
    "    \n",
    "    plt.imshow(overlap_vis)\n",
    "    plt.title('Overlap Visualization\\n(Red=GT, Green=Pred, Yellow=Overlap)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040d651",
   "metadata": {},
   "source": [
    "## 6. Lab Exercises\n",
    "\n",
    "### Exercise 1: Compare Traditional Methods\n",
    "Try different traditional segmentation methods on the same image and compare results.\n",
    "\n",
    "### Exercise 2: Parameter Tuning\n",
    "Experiment with different parameters for watershed segmentation (min_distance, threshold_abs).\n",
    "\n",
    "### Exercise 3: Evaluation\n",
    "Create a simple ground truth mask and evaluate different segmentation methods using the metrics we implemented.\n",
    "\n",
    "### Exercise 4: Multi-class Segmentation\n",
    "Modify the thresholding approach to create a 3-class segmentation (background, object1, object2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac939c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise solutions and additional experiments\n",
    "print(\"=== Lab Exercises ===\")\n",
    "print(\"\\nExercise 1: Method Comparison\")\n",
    "print(\"- Try atlas-based, thresholding, and watershed on your own images\")\n",
    "print(\"- Compare computational time and accuracy\")\n",
    "\n",
    "print(\"\\nExercise 2: Parameter Sensitivity\")\n",
    "print(\"- Vary watershed parameters and observe changes\")\n",
    "print(\"- Plot metrics vs parameter values\")\n",
    "\n",
    "print(\"\\nExercise 3: Create Ground Truth\")\n",
    "print(\"- Manually create a ground truth mask\")\n",
    "print(\"- Evaluate all methods against this ground truth\")\n",
    "\n",
    "print(\"\\nExercise 4: Multi-class Extension\")\n",
    "print(\"- Use multiple thresholds to create 3+ classes\")\n",
    "print(\"- Implement multi-class metrics (per-class IoU)\")\n",
    "\n",
    "# Template for students to fill in\n",
    "def student_exercise_template():\n",
    "    \"\"\"\n",
    "    Template for student exercises\n",
    "    \"\"\"\n",
    "    # TODO: Load your own image\n",
    "    # your_image = cv2.imread('path_to_your_image.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # TODO: Apply different methods\n",
    "    # method1_result = ...\n",
    "    # method2_result = ...\n",
    "    # method3_result = ...\n",
    "    \n",
    "    # TODO: Create or load ground truth\n",
    "    # ground_truth = ...\n",
    "    \n",
    "    # TODO: Calculate metrics for each method\n",
    "    # metrics1 = calculate_segmentation_metrics(method1_result, ground_truth)\n",
    "    # metrics2 = calculate_segmentation_metrics(method2_result, ground_truth)\n",
    "    \n",
    "    # TODO: Visualize and compare results\n",
    "    pass\n",
    "\n",
    "print(\"\\nUse the student_exercise_template() function as a starting point for your experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87159b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Conceptual Differences**: Object detection vs semantic vs instance segmentation\n",
    "2. **Annotation Tools**: Various tools for creating training data\n",
    "3. **Traditional Methods**: \n",
    "   - Atlas-based: Template matching and registration\n",
    "   - Thresholding: Otsu, adaptive, multi-level thresholding\n",
    "   - Watershed: Distance transform and marker-controlled watershed\n",
    "4. **Deep Learning**: Preprocessing, architectures, and post-processing\n",
    "5. **Evaluation**: Key metrics for assessing segmentation quality\n",
    "\n",
    "### Next Steps:\n",
    "- Practice with your own images\n",
    "- Explore deep learning frameworks (PyTorch, TensorFlow)\n",
    "- Try pre-trained models (torchvision, Detectron2)\n",
    "- Learn about 3D segmentation for medical imaging\n",
    "- Explore real-time segmentation techniques\n",
    "\n",
    "### Resources:\n",
    "- [U-Net Paper](https://arxiv.org/abs/1505.04597)\n",
    "- [Detectron2 Framework](https://github.com/facebookresearch/detectron2)\n",
    "- [Medical Segmentation Decathlon](http://medicaldecathlon.com/)\n",
    "- [COCO Dataset](https://cocodataset.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
